{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from llama_index.core import VectorStoreIndex, StorageContext, SimpleDirectoryReader, ServiceContext\n",
        "from llama_index.core import Settings\n",
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "from llama_index.llms.huggingface import HuggingFaceLLM\n",
        "from llama_index.core import PromptTemplate\n",
        "from llama_index.llms.openai import OpenAI\n",
        "import openai"
      ],
      "outputs": [],
      "execution_count": 17,
      "metadata": {
        "gather": {
          "logged": 1708692530952
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# openai.api_key = os.getenv(\"OPEN_AY_KEY\")\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# https://docs.llamaindex.ai/en/stable/examples/vector_stores/SimpleIndexDemoLlama-Local.html\n",
        "\n",
        "# https://blog.streamlit.io/build-a-chatbot-with-custom-data-sources-powered-by-llamaindex/\n",
        "\n",
        "# https://github.com/nicknochnack/Llama2RAG/blob/main/app.py"
      ],
      "outputs": [],
      "execution_count": 8,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1708687946532
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load the documents"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "documents = SimpleDirectoryReader(\"data\", recursive=True).load_data()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LLama2 model"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SYSTEM_PROMPT = \"\"\"You are an AI assistant that answers questions in a friendly manner, based on the given source documents. Here are some rules you always follow:\n",
        "- Generate human readable output, avoid creating output with gibberish text.\n",
        "- Keep your answers technical and based on facts, do not hallucinate features.\n",
        "- Generate only the requested output, don't include any other language before or after the requested output.\n",
        "- Never say thank you, that you are happy to help, that you are an AI agent, etc. Just answer directly.\n",
        "- Generate professional language.\n",
        "- Never generate offensive or foul language.\n",
        "\"\"\""
      ],
      "outputs": [],
      "execution_count": 21,
      "metadata": {
        "gather": {
          "logged": 1708693547530
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SYSTEM_PROMPT = \"\"\"Você é um assistente de IA que responde perguntas de maneira amigável, com base nos documentos de origem fornecidos. Aqui estão algumas regras que você sempre segue:\n",
        "- Gere resultados legíveis por humanos em pt-BR, evite criar resultados com texto sem sentido.\n",
        "- Mantenha suas respostas técnicas e baseadas em fatos, não tenha alucinações sobre as features.\n",
        "- Gere apenas a saída solicitada, não inclua nenhum outro idioma antes ou depois da saída solicitada.\n",
        "- Nunca diga obrigado, que você está feliz em ajudar, que é um agente de IA, etc. Basta responder diretamente.\n",
        "- Gerar linguagem profissional.\n",
        "- Nunca gere linguagem ofensiva ou chula\n",
        "\"\"\""
      ],
      "outputs": [],
      "execution_count": 35,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1708694249849
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query_wrapper_prompt = PromptTemplate(\n",
        "    \"[INST]<<SYS>>\\n\" + SYSTEM_PROMPT + \"<</SYS>>\\n\\n{query_str}[/INST] \"\n",
        ")"
      ],
      "outputs": [],
      "execution_count": 36,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1708694251968
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# llm = OpenAI(model=\"gpt-3.5-turbo\", temperature=0.5, system_prompt=SYSTEM_PROMPT)\n",
        "\n",
        "model_name =  \"meta-llama/Llama-2-7b-hf\"\n",
        "\n",
        "llm = HuggingFaceLLM(\n",
        "    context_window=4096,\n",
        "    max_new_tokens=2048,\n",
        "    generate_kwargs={\"temperature\": 0.0, \"do_sample\": False},\n",
        "    query_wrapper_prompt=query_wrapper_prompt,\n",
        "    tokenizer_name=model_name,\n",
        "    model_name=model_name,\n",
        "    device_map=\"auto\",\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1708696523891
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Embedding Model "
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load in a specific embedding model\n",
        "embed_model = HuggingFaceEmbedding(model_name='sentence-transformers/distiluse-base-multilingual-cased-v1')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Settings.llm = llm\n",
        "Settings.embed_model = embed_model"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# deprecated version\n",
        "# Create a service context with the custom embedding model\n",
        "# service_context = ServiceContext.from_defaults(llm=llm, chunk_size=800, chunk_overlap=20, embed_model=embed_model)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an index using the service context\n",
        "new_index = VectorStoreIndex.from_documents(documents)\n",
        "\n",
        "query_engine = new_index.as_query_engine()\n",
        "\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1708694001272
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = query_engine.query(\"sobre o que fala este documento?\")\n",
        "print(response)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1708693788908
        }
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "genai",
      "language": "python",
      "display_name": "genai"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      },
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "kernel_info": {
      "name": "genai"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}